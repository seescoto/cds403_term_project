#####################################
## Name: Sofia E. Escoto
## Assignment: 6
#####################################


# Using the same TESS data, build a forecasting model to predict RA value. 
# 1.	First, choose the dependent and independent variables for a linear regression model. Which
#variables did you choose and why? Is this a good model? (2 p)
# 2.	Second, build a regression tree model; is this a better model? (3 p)
# 3.	Third, build a model tree; is this a better model than the previous 2? (5 p)

#load packages
library(tidyverse) #sample_n
library(rpart) #regression tree rpart
library(RWeka) #model tree


#set wd and seed 
setwd("/Users/sofiaescoto/Desktop/CDS 403/week2/TESS archive")
set.seed(8)

#load data
temp <- list.files(pattern="*.csv")
myfiles <- lapply(temp, read.csv)
dataset <- do.call("rbind", myfiles)

dataset$Camera<- factor(dataset$Camera, levels = c("1", "2", "3", "4"),
                        labels = c("Camera1", "Camera2", "Camera3", "Camera4"))
dataset$CCD<- factor(dataset$CCD, levels = c("1", "2", "3", "4"), 
                     labels = c("CCD1", "CCD2", "CCD3", "CCD4"))

#subsetting and removing id
dataset <- sample_n(dataset, 50000)
dataset <- dataset[-1]  

###
# 1.	First, choose the dependent and independent variables for a linear regression model. Which
#variables did you choose and why? Is this a good model? (2 p)

#choosing depending variables... lets look at variables
cor(dataset$RA, y = dataset[c('Tmag', 'Dec')])
#tmag isn't very correlated but lets include it anyway

#independent variable: RA
#dependent variables: Camera, CCD, Tmag, Dec - aka all other features
#I know I want to predict RA so I should choose it to be the independent variable, and the
#dependent variables were chosen because I wanted to have as much information as possible
#to inform my prediction. Tmag didn't have a very big correlation with RA, but I wanted to
#keep it just to see if it would help since it can't hurt. If after I created the model, there
#was one or more variables that didnt have a low p-value in predicting RA, then I could remove
#them.

#80 to 20 split 
n <- count(dataset)[, 1]
sub1 <- n*.8
data_train <- dataset[1:sub1, ] #80%
data_test <- dataset[(sub1 + 1):n, ] #20%


regmod <- lm(RA ~ ., data = data_train)
regmod
summary(regmod)

#predictions 
regpred <- predict(regmod, data_test)
cor(regpred, data_test$RA)
#54% correlation

#mean absolute error 
MAE <- function(actual, predicted) {
  mean(abs(actual-predicted))
}

MAE(regpred, data_test$RA)
#62% error... oof

#this isn't a really good model. The adjusted R squared is only .29, meaning that this model 
#only explains 29% of the variance in RA. There is also only a 54% correlation between the
#values we predicted using the model and the actual RA values from the test set and a 62% error.
#The residuals are fairly balanced and the p-value shows that the model is statistically
#significant, but it won't help us too much if we want to accurately predict RA.


###
# 2.	Second, build a regression tree model; is this a better model? (3 p)

rtmod <- rpart(RA ~ ., data = data_train)
rtmod
summary(rtmod)

#predictions
rtpred <- predict(rtmod, data_test)
cor(rtpred, data_test$RA)
#71% correlation

MAE(rtpred, data_test$RA)
#only 47% error which is better.

#this model is better! Our predictions have a 71% correlation with the actual RA values in 
#the test set which is much better than 54%, and the error decreased from 62% with the
#regression model to 47% with the regression tree. This still isn't perfect, but it's better.


###
# 3.	Third, build a model tree; is this a better model than the previous 2? (5 p)

mpmod <- M5P(RA ~ ., data = data_train)
mpmod
summary(mpmod)

#predictions
mppred <- predict(mpmod, data_test)
cor(mppred, data_test$RA)
#79% correlation

MAE(mppred, data_test$RA)
#41% error

#Yes, this model is better than the previous 2. It's only a little bit better than the
#regression tree model, but much better than the linear regression model. This is because it 
#combines the tree and the lm by having the leaf nodes be linear regression models. It has an 
#error rate of about 41%, which is still not great, but much better than what we started with.

