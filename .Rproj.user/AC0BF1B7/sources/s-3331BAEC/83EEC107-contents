#####################################
## Name: Sofia E. Escoto
## Assignment: 5
#####################################

#Using the same TESS data from the previous homeworks, create and evaluate a decision tree model
#and a one rule model to predict which camera the instrument used based on the observations. Can
#you improve the decision tree model with boosting?

#load packages
library(C50) #decision tree
library(tidyverse) #for sample_n
library(gmodels) #crosstable
library(caret) #confusionMatrix
library(RWeka) #onerule/greedy learning

#set wd and seed 
setwd("/Users/sofiaescoto/Desktop/CDS 403/week2/TESS archive")
set.seed(8)

#load data
temp <- list.files(pattern="*.csv")
myfiles <- lapply(temp, read.csv)
dataset <- do.call("rbind", myfiles)

#factoring camera and ccd  
dataset$Camera<- factor(dataset$Camera, levels = c("1", "2", "3", "4"),
                        labels = c("Camera1", "Camera2", "Camera3", "Camera4"))
dataset$CCD<- factor(dataset$CCD, levels = c("1", "2", "3", "4"), 
                     labels = c("CCD1", "CCD2", "CCD3", "CCD4"))

#subsetting and removing id
dataset <- sample_n(dataset, 25000)
dataset <- dataset[-1]  

#decision trees don't need normalized data, so we can continue with it regularly
#make test and train sets
l<-length(row.names(dataset))
dataset_train <- dataset[1:round(0.7*l), ] 
dataset_test <- dataset[round((0.7*l)+1):l, ]
#save labels for camera
dataset_train_labels <- dataset[1:round(0.7*l), 1] 
dataset_test_labels <- dataset[round((0.7*l)+1):l, 1]

#is the data split proportionally beteween datasets?
prop.table(table(dataset_test$Camera))
prop.table(table(dataset_train$Camera))
#about the same proportions so this is good


#
###Decision tree (no boosting)
#
cam_model1 <- C5.0(dataset_train[-1], dataset_train_labels) 
#remove col 1 so camera isn't a feature in the model
cam_model1 #tree size is 286... that seems like a Lot of levels? I don't know?
summary(cam_model1)
#5.3% error rate but might just be overfitted because decision trees tend to do that

cam_pred1 <- predict(cam_model1, dataset_test)
CrossTable(dataset_test$Camera, cam_pred1, prop.c = FALSE, prop.r = FALSE, 
           dnn = c('actual camera', 'predicted camera'))
#looks pretty good?

confusionMatrix(cam_pred1, dataset_test$Camera)
#yeah looks good! 91% accuracy so not too overfitted from training data
#detection rate/detection prevalence aren't great but overall it looks good?


#
###One rule model
#
#greedy learners so don't need training/testing sets, can use dataset
cam_pred2 <- OneR(Camera ~ ., data = dataset)
summary(cam_pred2)
#definitely not as good as the decision tree, but I wonder why?
#maybe try JRip which finds the best rules to use?

cam_pred3 <- JRip(Camera ~ ., data = dataset)
summary(cam_pred3)
#much better than just one one rule, about the same as the decision tree without boosting


#
###Decision tree with boosting 
#
#lets try to improve our decision tree and see if we can tmag more
#because the original non-boosted one barely used it

cam_boost <- C5.0(dataset_train[-1], dataset_train_labels, trials = 10)
cam_boost
#about the same tree size as non-boosted one
summary(cam_boost)
#uses variables much more efficiently! non-boosted decision tree only used tmag ~1%, boosted 
#uses it almost 42%
#total error changed from 5.3% to 1.6%! that might just be overfitting though??

cam_pred_boost <- predict(cam_boost, dataset_test)
CrossTable(dataset_test_labels, cam_pred_boost, prop.c = FALSE, prop.r = FALSE, 
           dnn = c('actual camera', 'predicted camera'))
confusionMatrix(dataset_test_labels, cam_pred_boost)
#around the same accuracy as non-boosted even though it had a much lower error with the 
#training set
#that means the lower error was probably just because the boosting was overfitting the training
#data



###Explanation
#
#Even though the boosted decision tree model had lower error in the model summary, when used
#to predict labels for the test data, they had very similar accuracy and class statistics.
#I think the reason for this is because the boosting overfitted the model slightly to the
#training data, so it did really well with that, but it didn't have much of an effect on new
#data coming in (the test set).

